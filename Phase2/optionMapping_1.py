# import cv2
# import numpy as np
# import os
# import json

# class OptionMapper:
#     def __init__(self, image_path, annotations_path, classes_path, anchor_data):
#         self.image_path = image_path
#         self.annotations_path = annotations_path
#         self.classes_path = classes_path
        
#         self.anchor_data = anchor_data
        
#         self.original_image = cv2.imread(image_path)                                    # Keep original for loading annotations
#         if self.original_image is None:
#             raise FileNotFoundError(f"Image not found at {image_path}")
            
#         self.original_height, self.original_width = self.original_image.shape[:2]
#         self.classes = self._load_classes()
#         self.annotations = self._load_annotations()                                     # Annotations are still in original coordinates

#         '''
#         Load the deskewed image and M_transform
#         For simplicity, we'll assume anchorDetection already saved the deskewed image
#         or we re-deskew it here. Re-deskewing is safer for independent runs.
#         However, to maintain the deskewed image generated by anchorDetection,
#         it's best to pass it directly if the scripts run sequentially.
#         For a more robust solution, the deskewed image should ideally be loaded or generated here.
#         Given your current setup, we'll assume the image passed to OptionMapper
#         will eventually be the deskewed one or we will apply the transform.
#         '''

#         # We need the transformation matrix to convert original annotation coordinates
#         self.M_transform = np.array(self.anchor_data.get("M_transform")) if self.anchor_data.get("M_transform") else None
#         self.deskewed_width = self.anchor_data.get("deskewed_width", self.original_width)
#         self.deskewed_height = self.anchor_data.get("deskewed_height", self.original_height)

#         # The image that we will draw on should be the deskewed one.
#         # Let's re-deskew the image here using the stored M_transform.
#         if self.M_transform is not None:
#             self.image = cv2.warpPerspective(self.original_image, self.M_transform, (self.deskewed_width, self.deskewed_height))
#         else:
#             self.image = self.original_image.copy() # If no deskewing, just use the original image copy


#     def _load_classes(self):
#         with open(self.classes_path, 'r') as f:
#             return [line.strip() for line in f.readlines()]

#     def _load_annotations(self):
#         annotations = []
#         with open(self.annotations_path, 'r') as f:
#             for line in f:
#                 parts = line.strip().split()
#                 if len(parts) == 5:
#                     class_id = int(parts[0])
#                     # Store normalized coordinates
#                     norm_x_center = float(parts[1])
#                     norm_y_center = float(parts[2])
#                     norm_width = float(parts[3])
#                     norm_height = float(parts[4])
#                     annotations.append((self.classes[class_id], norm_x_center, norm_y_center, norm_width, norm_height))
#         return annotations
    
#     def map_and_draw(self):
#         # The reference anchor (Anch1) center in the DESKEWED image
#         ref_x, ref_y = self.anchor_data["anchors"]["Anch1"]

#         for class_name, norm_x_center, norm_y_center, norm_width, norm_height in self.annotations:
#             if "Anch" in class_name:
#                 continue

#             # Convert normalized original coordinates to pixel original coordinates
#             original_x_center = norm_x_center * self.original_width
#             original_y_center = norm_y_center * self.original_height
#             original_width_px = norm_width * self.original_width
#             original_height_px = norm_height * self.original_height

#             # Calculate original bounding box corners
#             original_x1 = original_x_center - original_width_px / 2
#             original_y1 = original_y_center - original_height_px / 2
#             original_x2 = original_x_center + original_width_px / 2
#             original_y2 = original_y_center + original_height_px / 2

#             if self.M_transform is not None:
#                 # Transform the original bounding box corners to the deskewed image space
#                 original_pts = np.float32([
#                     [original_x1, original_y1], [original_x2, original_y1],
#                     [original_x2, original_y2], [original_x1, original_y2]
#                 ]).reshape(-1, 1, 2)
#                 transformed_pts = cv2.perspectiveTransform(original_pts, self.M_transform).reshape(-1, 2)

#                 # Get the new bounding box in the deskewed image
#                 new_x1 = int(np.min(transformed_pts[:, 0]))
#                 new_y1 = int(np.min(transformed_pts[:, 1]))
#                 new_x2 = int(np.max(transformed_pts[:, 0]))
#                 new_y2 = int(np.max(transformed_pts[:, 1]))
#             else:
#                 # If no deskewing, use original coordinates as is
#                 new_x1, new_y1, new_x2, new_y2 = int(original_x1), int(original_y1), int(original_x2), int(original_y2)

#             cv2.rectangle(self.image, (new_x1, new_y1), (new_x2, new_y2), (0, 255, 0), 2)
#             cv2.putText(self.image, class_name, (new_x1, new_y1 - 5),
#                         cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 2)

#         return self.image

# def process_folder(folder_path, annotations_file, classes_file, anchor_centers_path):
#     folder_name = os.path.basename(folder_path.rstrip("\\/"))
#     output_dir = f"options_{folder_name}"
#     warning_dir = os.path.join(output_dir, "warnings")
#     os.makedirs(output_dir, exist_ok=True)
#     os.makedirs(warning_dir, exist_ok=True)

#     with open(anchor_centers_path, "r") as f:
#         all_anchor_data = json.load(f) # Load all data

#     for filename in os.listdir(folder_path):
#         if filename.lower().endswith((".jpg", ".jpeg", ".png")):
#             image_path = os.path.join(folder_path, filename)
#             print(f"\nProcessing {image_path}...")

#             try:
#                 if filename not in all_anchor_data:
#                     raise Exception("Anchor data not found for this image.")
                
#                 image_specific_anchor_data = all_anchor_data[filename]

#                 mapper = OptionMapper(image_path, annotations_file, classes_file, image_specific_anchor_data)
#                 mapped_image = mapper.map_and_draw()

#                 save_path = os.path.join(output_dir, filename)
#                 cv2.imwrite(save_path, mapped_image)
#                 print(f"Mapped and saved to {save_path}")

#             except Exception as e:
#                 print(f"Error processing {filename}: {e}")
#                 warning_path = os.path.join(warning_dir, filename)
#                 # Save the original image to warnings if processing fails
#                 cv2.imwrite(warning_path, cv2.imread(image_path)) 
#                 print(f"Saved to warning folder: {warning_path}")

# if __name__ == "__main__":
#     folder_path = r"D:\Projects\OMR\new_abhigyan\Phase1\testData\Option_Checking"
#     annotations_file = r"D:\Projects\OMR\new_abhigyan\Phase1\Options\BE23-01-01003.txt"
#     classes_file = r"D:\Projects\OMR\new_abhigyan\Phase1\Options\classes.txt"
    
#     # Path to the anchor centers JSON file created after anchorDetection.py
#     anchor_centers_path = r"D:\Projects\OMR\new_abhigyan\Phase2\output_Option_Checking\anchor_centers.json" 

#     process_folder(folder_path, annotations_file, classes_file, anchor_centers_path)



# VERSION 1.1

import cv2
import numpy as np
import os
import json
import re

class MarkedOptionDetector:
    def __init__(self, image_path, mapped_annotations_data, output_dir):
        self.image_path = image_path
        self.image = cv2.imread(image_path) 
        if self.image is None:
            raise FileNotFoundError(f"Image not found at {image_path}. Ensure this is the deskewed image generated by optionMapping.py.")
        
        self.mapped_annotations = mapped_annotations_data
        self.output_dir = output_dir

        self.option_crops_dir = os.path.join(self.output_dir, "option_crops")
        os.makedirs(self.option_crops_dir, exist_ok=True)

        self.marked_superimposed_dir = os.path.join(self.output_dir, "marked_superimposed")
        os.makedirs(self.marked_superimposed_dir, exist_ok=True)


    def _extract_question_option_bboxes(self):
        questions = {}
        options = {}

        for class_name, bbox in self.mapped_annotations.items():
            match_q = re.match(r'Q(\d+)', class_name)
            match_opt = re.match(r'(\d+)([A-D])', class_name)

            if match_q:
                q_num = int(match_q.group(1))
                questions[q_num] = bbox
            elif match_opt:
                q_num = int(match_opt.group(1))
                option_letter = match_opt.group(2)
                if q_num not in options:
                    options[q_num] = {}
                options[q_num][option_letter] = bbox
        
        for q_num in options:
            options[q_num] = dict(sorted(options[q_num].items()))

        return questions, options

    # --- REFINED MARKED PERCENTAGE DETECTION WITH ROBUST CIRCLE EXTRACTION ---
    def _get_marked_data_from_circle(self, option_img_roi, min_pixel_intensity_threshold=90): # Default back to 90 for mean intensity
        gray_option = cv2.cvtColor(option_img_roi, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray_option, (5, 5), 0)

        # Attempt to find circles with more forgiving parameters
        min_dim = min(option_img_roi.shape[0], option_img_roi.shape[1])
        min_r_estimate = int(min_dim * 0.3)
        max_r_estimate = int(min_dim * 0.45)
        
        # Use slightly adjusted HoughCircles parameters
        # dp: Increase if circles are still not found, but can lead to false positives. 1.0 is highest resolution.
        # minDist: Can be made smaller if circles are very close.
        # param1: Higher for stronger edges.
        # param2: Lower for more circles (more false positives but fewer misses). Try from 20-30.
        circles = cv2.HoughCircles(blurred, cv2.HOUGH_GRADIENT, dp=1.0, minDist=min_dim // 4, # Smaller minDist
                                   param1=100, param2=20, # Reduced param2
                                   minRadius=min_r_estimate, maxRadius=max_r_estimate)

        circle_mask = np.zeros_like(gray_option)
        center_x, center_y, radius = -1, -1, -1 # Default values if no circle found
        detected_circle = False

        if circles is not None:
            circles = np.uint16(np.around(circles))
            largest_circle = None
            max_radius = 0
            for i in circles[0, :]:
                # Filter out circles too close to the border or too small
                # This helps prevent finding the bounding box edges as circles
                if i[2] > max_radius and \
                   i[0] - i[2] >= 0 and i[0] + i[2] < option_img_roi.shape[1] and \
                   i[1] - i[2] >= 0 and i[1] + i[2] < option_img_roi.shape[0]:
                    max_radius = i[2]
                    largest_circle = i
            
            if largest_circle is not None:
                center_x, center_y, radius = largest_circle[0], largest_circle[1], largest_circle[2]
                cv2.circle(circle_mask, (center_x, center_y), radius, 255, -1)
                detected_circle = True

        if not detected_circle:
            print("Warning: No prominent circle found. Analyzing entire ROI.")
            # Fallback: if no circle, use the central square region of the ROI as the effective area
            # This is better than the entire rectangle for fill percentage
            h, w = gray_option.shape
            square_side = min(h, w)
            x_start = (w - square_side) // 2
            y_start = (h - square_side) // 2
            circle_mask[y_start : y_start + square_side, x_start : x_start + square_side] = 255
            # For visualization, pretend a circle centered in this square was found
            center_x, center_y = w // 2, h // 2
            radius = square_side // 2

        # Apply the mask to the blurred grayscale image
        masked_region = cv2.bitwise_and(blurred, blurred, mask=circle_mask)
        
        pixels_in_mask = masked_region[circle_mask == 255]
        
        if pixels_in_mask.size == 0:
            return 0.0, np.zeros_like(blurred), 255.0, False, (center_x, center_y, radius), detected_circle

        mean_intensity = np.mean(pixels_in_mask)
        is_potentially_marked_by_intensity = mean_intensity < min_pixel_intensity_threshold
        
        # --- Robust Binarization for Fill Percentage ---
        # Instead of relying solely on Otsu on the masked region, let's use the overall ROI for thresholding
        # And then apply the mask. This can sometimes give a better global threshold.
        # Alternative: use a fixed threshold or adaptive threshold on the masked region if Otsu struggles.
        
        # Let's try Otsu on the original blurred ROI, then apply mask.
        # This gives a threshold that considers the full range of pixel values in the box.
        _, global_thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # Apply the circle mask to this global threshold
        thresh_binary_masked = cv2.bitwise_and(global_thresh, global_thresh, mask=circle_mask)

        # Morphological operations to clean up
        kernel = np.ones((3,3), np.uint8) 
        thresh_eroded = cv2.erode(thresh_binary_masked, kernel, iterations=1)
        thresh_dilated = cv2.dilate(thresh_eroded, kernel, iterations=2) 
        
        # The final thresholded image used for counting. Ensure it's within the mask.
        thresh_final = cv2.bitwise_and(thresh_dilated, thresh_dilated, mask=circle_mask)

        marked_pixels_count = cv2.countNonZero(thresh_final)
        total_mask_pixels = cv2.countNonZero(circle_mask)

        fill_percentage = 0.0
        if total_mask_pixels > 0:
            fill_percentage = marked_pixels_count / total_mask_pixels

        return fill_percentage, thresh_final, mean_intensity, is_potentially_marked_by_intensity, (center_x, center_y, radius), detected_circle
    # --- END REFINED MARKED PERCENTAGE DETECTION ---


    def detect_marked_options(self, min_fill_percentage_threshold=0.35, min_pixel_intensity_threshold=90): # Adjusted default thresholds
        questions_bboxes, options_bboxes = self._extract_question_option_bboxes()
        results = {}
        
        image_with_marked_overlays = self.image.copy()

        print("\n--- Option Analysis ---")
        print(f"Global min_fill_percentage_threshold: {min_fill_percentage_threshold:.2f}")
        print(f"Global min_pixel_intensity_threshold: {min_pixel_intensity_threshold:.1f}\n")


        for q_num in sorted(questions_bboxes.keys()): # Iterate through questions to ensure order
            marked_options_for_q = []
            
            if q_num in options_bboxes:
                for option_letter, bbox in options_bboxes[q_num].items():
                    x1, y1, x2, y2 = bbox
                    
                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(self.image.shape[1], x2)
                    y2 = min(self.image.shape[0], y2)

                    if x2 <= x1 or y2 <= y1:
                        print(f"Warning: Invalid bounding box for {q_num}{option_letter}: {bbox}. Skipping.")
                        continue

                    option_img_roi = self.image[y1:y2, x1:x2]

                    if option_img_roi.size == 0:
                        print(f"Warning: Empty ROI for {q_num}{option_letter}. Skipping.")
                        continue

                    # Call the refined detection function
                    fill_percentage, thresh_img, mean_intensity, is_potentially_marked_by_intensity, circle_coords, circle_found = \
                        self._get_marked_data_from_circle(option_img_roi, min_pixel_intensity_threshold)
                    
                    # Determine if marked based on both criteria (fill percentage AND initial intensity check)
                    is_marked = (fill_percentage >= min_fill_percentage_threshold) and is_potentially_marked_by_intensity
                    
                    print(f"  Q{q_num}{option_letter}: Fill={fill_percentage:.2f}, MeanIntensity={mean_intensity:.1f}, "
                          f"PotentiallyMarkedByIntensity={is_potentially_marked_by_intensity}, IsMarked={is_marked}")

                    # Save individual option images and their thresholded versions
                    base_filename = os.path.basename(self.image_path)
                    name_without_ext = os.path.splitext(base_filename)[0]
                    
                    # Highlight the detected circle (or square fallback) on the original crop for debugging
                    vis_roi = option_img_roi.copy()
                    cx, cy, r = circle_coords
                    if circle_found and r > 0:
                        cv2.circle(vis_roi, (cx, cy), r, (0, 255, 0), 1) # Green circle
                    elif not circle_found: # Draw a red rectangle for the fallback square region
                        h, w = option_img_roi.shape[:2]
                        square_side = min(h, w)
                        x_start = (w - square_side) // 2
                        y_start = (h - square_side) // 2
                        cv2.rectangle(vis_roi, (x_start, y_start), (x_start + square_side, y_start + square_side), (0, 0, 255), 1)

                    option_crop_filename = os.path.join(self.option_crops_dir, 
                                                        f"{name_without_ext}_Q{q_num}{option_letter}_meanI_{mean_intensity:.1f}_fill_{fill_percentage:.2f}.jpg")
                    cv2.imwrite(option_crop_filename, vis_roi)

                    # Save the cleaned-up thresholded image
                    thresh_crop_filename = os.path.join(self.option_crops_dir, 
                                                        f"{name_without_ext}_Q{q_num}{option_letter}_thresh_cleaned.jpg")
                    cv2.imwrite(thresh_crop_filename, thresh_img)


                    # Prepare text for visualization on the main image
                    # display_text = f"{option_letter}: {fill_percentage:.2f} ({mean_intensity:.1f})"
                    display_text = f"    {fill_percentage:.2f}"
                    text_color = (0, 0, 255) # Red for unmarked
                    
                    if is_marked:
                        marked_options_for_q.append(option_letter)
                        cv2.rectangle(image_with_marked_overlays, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green for marked
                        text_color = (0, 255, 0) # Green text for marked
                        # display_text += " MARKED"
                    else:
                        cv2.rectangle(image_with_marked_overlays, (x1, y1), (x2, y2), (0, 0, 255), 1) # Red for unmarked, thinner line

                    text_x = x1
                    text_y = y1 - 5
                    if text_y < 0: text_y = y1 + 15
                    if text_x < 0: text_x = 0
                    
                    cv2.putText(image_with_marked_overlays, display_text, (text_x, text_y), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.4, text_color, 1)
            
            results[q_num] = marked_options_for_q
        
        marked_overlay_filename = os.path.join(self.marked_superimposed_dir, os.path.basename(self.image_path))
        cv2.imwrite(marked_overlay_filename, image_with_marked_overlays)

        return results

def process_marked_options_folder(folder_path, mapped_annotations_path, output_base_dir):
    with open(mapped_annotations_path, 'r') as f:
        all_mapped_annotations_data = json.load(f)

    marked_output_dir = os.path.join(output_base_dir, "marked_results")
    os.makedirs(marked_output_dir, exist_ok=True)

    final_results = {}

    for filename in os.listdir(folder_path):
        if filename.lower().endswith((".jpg", ".jpeg", ".png")):
            deskewed_image_dir = f"options_{os.path.basename(folder_path.rstrip('/\\'))}"
            deskewed_image_path = os.path.join(output_base_dir, deskewed_image_dir, filename)
            
            print(f"\nDetecting marked options for {deskewed_image_path}...")

            if not os.path.exists(deskewed_image_path):
                print(f"Skipping {filename}: Deskewed image not found at {deskewed_image_path}. Make sure optionMapping.py ran successfully.")
                continue

            if filename not in all_mapped_annotations_data:
                print(f"Skipping {filename}: Mapped annotations not found.")
                continue

            image_mapped_annotations = all_mapped_annotations_data[filename]
            
            try:
                detector = MarkedOptionDetector(deskewed_image_path, image_mapped_annotations, marked_output_dir)
                # Recommended starting thresholds. Tune based on output.
                marked_options_per_q = detector.detect_marked_options(
                    min_fill_percentage_threshold=0.35, # Reduced for more flexibility, will tune with new fill values
                    min_pixel_intensity_threshold=90    # Retained, as this looked promising for dark areas
                ) 
                final_results[filename] = marked_options_per_q

                print(f"\nResults for {filename}:")
                for q_num in sorted(marked_options_per_q.keys()):
                    marked = marked_options_per_q[q_num]
                    if marked:
                        print(f"  Q{q_num}: {', '.join(marked)}")
                    else:
                        print(f"  Q{q_num}: (No option marked)")

            except Exception as e:
                print(f"Error detecting marked options for {filename}: {e}")
                error_img_path = os.path.join(marked_output_dir, "warnings")
                os.makedirs(error_img_path, exist_ok=True)
                cv2.imwrite(os.path.join(error_img_path, filename), cv2.imread(deskewed_image_path))


    final_results_json_path = os.path.join(marked_output_dir, "marked_options_summary.json")
    with open(final_results_json_path, 'w') as f:
        json.dump(final_results, f, indent=2)
    print(f"\nSummary of marked options saved to {final_results_json_path}")

if __name__ == "__main__":
    folder_path = r"D:\Projects\OMR\new_abhigyan\Phase1\testData\Option_Checking" 
    mapped_annotations_path = r"D:\Projects\OMR\new_abhigyan\Phase2\options_Option_Checking\mapped_annotations.json" 
    output_base_dir = r"D:\Projects\OMR\new_abhigyan\Phase2" 

    process_marked_options_folder(folder_path, mapped_annotations_path, output_base_dir)